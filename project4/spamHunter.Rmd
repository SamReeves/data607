---
title: "Spam Hunter"
author: "Sam Reeves"
date: "4/29/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### The Project

We have been tasked with creating a model to correctly separate spam emails from ham emails.  Apparently, ham is industry jargon for the emails you actually want to receive.

We will try to build a model using boilerplate text analysis.  First, creating a corpus and document term matrix from the bank of labeled emails.

### The Data

SpamAssassin is an open-source project from Apache meant to enable system administrators to implement text analysis, Bayesian filtering, and other methods to help stamp out the global spamdemic.

Version 3.4.6 was realeased on April 12 of this year, and it is the last update before the release of 4.0.  I found it helpful to read through the source code.  You guessed it, this release comes with an Apache v2.0 license:

https://downloads.apache.org//spamassassin/source/Mail-SpamAssassin-3.4.6.tar.gz

We will be loading some old datasets and not using this software.  It's helpful to read through the source code.  The data can be found here:

https://spamassassin.apache.org/old/publiccorpus/

```{r, message=FALSE}
library(tidyverse)
library(magrittr)
library(tidytext)
library(tm)

hamURL <- "https://github.com/TheWerefriend/data607/raw/master/project4/ham.zip"
spamURL <- "https://github.com/TheWerefriend/data607/raw/master/project4/spam.zip"

collectMail <- function(zipURL, spamStatus) {
  dir <- getwd()
  temp <- tempfile()
  download.file(zipURL, temp)
  
  file_list <- unzip(temp, list = TRUE)
  unzip(temp)
  
  df <- list.files(path = dir) %>%
  as.data.frame() %>%
  set_colnames("filename") %>%
  mutate(text = lapply(list.files(path = dir,
                                  full.names = TRUE),
                       read_lines)) %>%
  unnest(c(text)) %>%
  mutate(type = spamStatus) %>%
  group_by(filename) %>%
  mutate(text = paste(text, collapse = " ")) %>%
  ungroup() %>%
  distinct()
  
  unlink(temp)
  do.call(file.remove, file_list)
  return(tibble(df))
}
```

Here we just call the collectMail function on each zip file to create tibbles of spam and ham email.

```{r, message=FALSE, warning=FALSE}
#ham <- collectMail(hamURL, spamStatus = 0)
spam <- collectMail(spamURL, spamStatus = 1)
```

We want to create a document term matrix using the tm package (like in the Eric Cartman meets Reddit project...) and use this to train our model.

```{r}
transformMail <- function(mail) {
  transformedCorpus <- VCorpus(VectorSource(mail$text)) %>%
  tm_map(removePunctuation, ucp = TRUE) %>%
  tm_map(removeNumbers) %>%
  tm_map(content_transformer(tolower)) %>%
  tm_map(removeWords, stopwords("SMART")) %>%
  stemDocument()
  
  return(transformedCorpus)
  }

```

```{r}
spamCorpus <- transformMail(spam)
#hamCorpus <- transformMail(ham)
```


### The Model

### Binary Categorization

### Conclusions